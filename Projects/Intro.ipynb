{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine Learning Series\n",
    "===================\n",
    "\n",
    "\n",
    "-----\n",
    "\n",
    "#1 - Install **sklearn** and **quandl**\n",
    "======\n",
    "\n",
    "- pip install sklearn \n",
    "\n",
    "- pip install quandl\n",
    "\n",
    "- When using Jupyter Notebook use the following terminal entry \"**conda install -c https://conda.anaconda.org/anaconda quandl**\"\n",
    "\n",
    "- Make sure \"**urllib3**\" is installed as well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn as sk\n",
    "import pandas as pd\n",
    "import quandl, math, datetime\n",
    "import numpy as np\n",
    "from sklearn import preprocessing, model_selection, svm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Make sure to insert API key below! You can get one by creating an account on www.quandl.com**\n",
    "\n",
    "*If you upload your Notebook to any public place make sure you **delete** your API key!!!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "quandl.ApiConfig.api_key = \"bsPqzouaLS1qBLUMqk2M\"\n",
    "df = quandl.get('WIKI/GOOGL') #quandl.get returns a dataframe!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Open     High      Low     Close      Volume  Ex-Dividend  \\\n",
      "Date                                                                        \n",
      "2004-08-19   100.010   104.06    95.96   100.335  44659000.0          0.0   \n",
      "2004-08-20   101.010   109.08   100.50   108.310  22834300.0          0.0   \n",
      "2004-08-23   110.760   113.48   109.05   109.400  18256100.0          0.0   \n",
      "2004-08-24   111.240   111.60   103.57   104.870  15247300.0          0.0   \n",
      "2004-08-25   104.760   108.00   103.88   106.000   9188600.0          0.0   \n",
      "2004-08-26   104.950   107.95   104.66   107.910   7094800.0          0.0   \n",
      "2004-08-27   108.100   108.62   105.69   106.150   6211700.0          0.0   \n",
      "2004-08-30   105.280   105.49   102.01   102.010   5196700.0          0.0   \n",
      "2004-08-31   102.320   103.71   102.16   102.370   4917800.0          0.0   \n",
      "2004-09-01   102.700   102.97    99.67   100.250   9138200.0          0.0   \n",
      "2004-09-02    99.090   102.37    98.94   101.510  15118600.0          0.0   \n",
      "2004-09-03   100.950   101.74    99.32   100.010   5152400.0          0.0   \n",
      "2004-09-07   101.010   102.00    99.61   101.580   5847500.0          0.0   \n",
      "2004-09-08   100.740   103.03   100.50   102.300   4985600.0          0.0   \n",
      "2004-09-09   102.500   102.71   101.00   102.310   4061700.0          0.0   \n",
      "2004-09-10   101.470   106.56   101.30   105.330   8698800.0          0.0   \n",
      "2004-09-13   106.630   108.41   106.46   107.500   7844100.0          0.0   \n",
      "2004-09-14   107.440   112.00   106.79   111.490  10828900.0          0.0   \n",
      "2004-09-15   110.560   114.23   110.20   112.000  10713000.0          0.0   \n",
      "2004-09-16   112.340   115.80   111.65   113.970   9266300.0          0.0   \n",
      "2004-09-17   114.420   117.49   113.55   117.490   9472500.0          0.0   \n",
      "2004-09-20   116.950   121.60   116.77   119.360  10628700.0          0.0   \n",
      "2004-09-21   120.200   120.42   117.51   117.840   7228700.0          0.0   \n",
      "2004-09-22   117.450   119.67   116.81   118.380   7581200.0          0.0   \n",
      "2004-09-23   118.840   122.63   117.02   120.820   8535600.0          0.0   \n",
      "2004-09-24   120.970   124.10   119.76   119.830   9123400.0          0.0   \n",
      "2004-09-27   119.560   120.88   117.80   118.260   7066100.0          0.0   \n",
      "2004-09-28   121.150   127.40   120.21   126.860  16929000.0          0.0   \n",
      "2004-09-29   126.530   135.02   126.23   131.080  30516400.0          0.0   \n",
      "2004-09-30   129.899   132.30   129.00   129.600  13758000.0          0.0   \n",
      "...              ...      ...      ...       ...         ...          ...   \n",
      "2018-02-13  1050.000  1061.22  1046.92  1054.140   1574121.0          0.0   \n",
      "2018-02-14  1054.320  1075.47  1049.80  1072.700   2029979.0          0.0   \n",
      "2018-02-15  1083.450  1094.10  1067.23  1091.360   1806206.0          0.0   \n",
      "2018-02-16  1093.380  1108.31  1091.55  1095.500   1971928.0          0.0   \n",
      "2018-02-20  1092.760  1116.29  1090.00  1103.590   1646405.0          0.0   \n",
      "2018-02-21  1109.100  1136.20  1107.51  1113.750   2024534.0          0.0   \n",
      "2018-02-22  1119.170  1125.46  1105.15  1109.900   1386115.0          0.0   \n",
      "2018-02-23  1118.660  1129.00  1108.44  1128.090   1234539.0          0.0   \n",
      "2018-02-26  1131.860  1144.20  1129.34  1143.700   1489118.0          0.0   \n",
      "2018-02-27  1143.700  1144.25  1116.79  1117.510   2094863.0          0.0   \n",
      "2018-02-28  1122.000  1127.65  1103.00  1103.920   2431023.0          0.0   \n",
      "2018-03-01  1109.540  1111.27  1067.29  1071.410   2766856.0          0.0   \n",
      "2018-03-02  1057.980  1086.89  1050.11  1084.140   2508145.0          0.0   \n",
      "2018-03-05  1078.130  1101.18  1072.27  1094.760   1432369.0          0.0   \n",
      "2018-03-06  1102.100  1105.63  1094.50  1100.900   1169068.0          0.0   \n",
      "2018-03-07  1092.820  1116.20  1089.91  1115.040   1537429.0          0.0   \n",
      "2018-03-08  1117.200  1131.44  1117.20  1129.380   1510478.0          0.0   \n",
      "2018-03-09  1139.500  1161.00  1134.29  1160.840   2070174.0          0.0   \n",
      "2018-03-12  1165.050  1178.16  1159.20  1165.930   2129297.0          0.0   \n",
      "2018-03-13  1171.830  1178.00  1134.57  1139.910   2129435.0          0.0   \n",
      "2018-03-14  1145.800  1159.76  1142.35  1148.890   2033697.0          0.0   \n",
      "2018-03-15  1149.570  1162.50  1135.66  1150.610   1623868.0          0.0   \n",
      "2018-03-16  1155.350  1156.81  1131.36  1134.420   2654602.0          0.0   \n",
      "2018-03-19  1117.760  1119.37  1088.92  1100.070   3076349.0          0.0   \n",
      "2018-03-20  1098.400  1105.55  1082.42  1095.800   2709310.0          0.0   \n",
      "2018-03-21  1092.570  1108.70  1087.21  1094.000   1990515.0          0.0   \n",
      "2018-03-22  1080.010  1083.92  1049.64  1053.150   3418154.0          0.0   \n",
      "2018-03-23  1051.370  1066.78  1024.87  1026.550   2413517.0          0.0   \n",
      "2018-03-26  1050.600  1059.27  1010.58  1054.090   3272409.0          0.0   \n",
      "2018-03-27  1063.900  1064.54   997.62  1006.940   2940957.0          0.0   \n",
      "\n",
      "            Split Ratio    Adj. Open    Adj. High     Adj. Low   Adj. Close  \\\n",
      "Date                                                                          \n",
      "2004-08-19          1.0    50.159839    52.191109    48.128568    50.322842   \n",
      "2004-08-20          1.0    50.661387    54.708881    50.405597    54.322689   \n",
      "2004-08-23          1.0    55.551482    56.915693    54.693835    54.869377   \n",
      "2004-08-24          1.0    55.792225    55.972783    51.945350    52.597363   \n",
      "2004-08-25          1.0    52.542193    54.167209    52.100830    53.164113   \n",
      "2004-08-26          1.0    52.637487    54.142132    52.492038    54.122070   \n",
      "2004-08-27          1.0    54.217364    54.478169    53.008633    53.239345   \n",
      "2004-08-30          1.0    52.802998    52.908323    51.162935    51.162935   \n",
      "2004-08-31          1.0    51.318415    52.015567    51.238167    51.343492   \n",
      "2004-09-01          1.0    51.509003    51.644421    49.989312    50.280210   \n",
      "2004-09-02          1.0    49.698414    51.343492    49.623182    50.912161   \n",
      "2004-09-03          1.0    50.631294    51.027517    49.813770    50.159839   \n",
      "2004-09-07          1.0    50.661387    51.157920    49.959219    50.947269   \n",
      "2004-09-08          1.0    50.525969    51.674514    50.405597    51.308384   \n",
      "2004-09-09          1.0    51.408694    51.514019    50.656371    51.313400   \n",
      "2004-09-10          1.0    50.892099    53.444980    50.806836    52.828075   \n",
      "2004-09-13          1.0    53.480088    54.372844    53.394825    53.916435   \n",
      "2004-09-14          1.0    53.886342    56.173402    53.560336    55.917612   \n",
      "2004-09-15          1.0    55.451172    57.291854    55.270615    56.173402   \n",
      "2004-09-16          1.0    56.343928    58.079285    55.997860    57.161452   \n",
      "2004-09-17          1.0    57.387149    58.926902    56.950802    58.926902   \n",
      "2004-09-20          1.0    58.656066    60.988265    58.565787    59.864797   \n",
      "2004-09-21          1.0    60.286097    60.396438    58.936933    59.102444   \n",
      "2004-09-22          1.0    58.906840    60.020277    58.585849    59.373280   \n",
      "2004-09-23          1.0    59.603992    61.504860    58.691174    60.597057   \n",
      "2004-09-24          1.0    60.672290    62.242136    60.065416    60.100525   \n",
      "2004-09-27          1.0    59.965107    60.627150    59.082382    59.313094   \n",
      "2004-09-28          1.0    60.762568    63.897245    60.291113    63.626409   \n",
      "2004-09-29          1.0    63.460898    67.719042    63.310433    65.742942   \n",
      "2004-09-30          1.0    65.150614    66.354831    64.699722    65.000651   \n",
      "...                 ...          ...          ...          ...          ...   \n",
      "2018-02-13          1.0  1050.000000  1061.220000  1046.920000  1054.140000   \n",
      "2018-02-14          1.0  1054.320000  1075.470000  1049.800000  1072.700000   \n",
      "2018-02-15          1.0  1083.450000  1094.100000  1067.230000  1091.360000   \n",
      "2018-02-16          1.0  1093.380000  1108.310000  1091.550000  1095.500000   \n",
      "2018-02-20          1.0  1092.760000  1116.290000  1090.000000  1103.590000   \n",
      "2018-02-21          1.0  1109.100000  1136.200000  1107.510000  1113.750000   \n",
      "2018-02-22          1.0  1119.170000  1125.460000  1105.150000  1109.900000   \n",
      "2018-02-23          1.0  1118.660000  1129.000000  1108.440000  1128.090000   \n",
      "2018-02-26          1.0  1131.860000  1144.200000  1129.340000  1143.700000   \n",
      "2018-02-27          1.0  1143.700000  1144.250000  1116.790000  1117.510000   \n",
      "2018-02-28          1.0  1122.000000  1127.650000  1103.000000  1103.920000   \n",
      "2018-03-01          1.0  1109.540000  1111.270000  1067.290000  1071.410000   \n",
      "2018-03-02          1.0  1057.980000  1086.890000  1050.110000  1084.140000   \n",
      "2018-03-05          1.0  1078.130000  1101.180000  1072.270000  1094.760000   \n",
      "2018-03-06          1.0  1102.100000  1105.630000  1094.500000  1100.900000   \n",
      "2018-03-07          1.0  1092.820000  1116.200000  1089.910000  1115.040000   \n",
      "2018-03-08          1.0  1117.200000  1131.440000  1117.200000  1129.380000   \n",
      "2018-03-09          1.0  1139.500000  1161.000000  1134.290000  1160.840000   \n",
      "2018-03-12          1.0  1165.050000  1178.160000  1159.200000  1165.930000   \n",
      "2018-03-13          1.0  1171.830000  1178.000000  1134.570000  1139.910000   \n",
      "2018-03-14          1.0  1145.800000  1159.760000  1142.350000  1148.890000   \n",
      "2018-03-15          1.0  1149.570000  1162.500000  1135.660000  1150.610000   \n",
      "2018-03-16          1.0  1155.350000  1156.810000  1131.360000  1134.420000   \n",
      "2018-03-19          1.0  1117.760000  1119.370000  1088.920000  1100.070000   \n",
      "2018-03-20          1.0  1098.400000  1105.550000  1082.420000  1095.800000   \n",
      "2018-03-21          1.0  1092.570000  1108.700000  1087.210000  1094.000000   \n",
      "2018-03-22          1.0  1080.010000  1083.920000  1049.640000  1053.150000   \n",
      "2018-03-23          1.0  1051.370000  1066.780000  1024.870000  1026.550000   \n",
      "2018-03-26          1.0  1050.600000  1059.270000  1010.580000  1054.090000   \n",
      "2018-03-27          1.0  1063.900000  1064.540000   997.620000  1006.940000   \n",
      "\n",
      "            Adj. Volume  \n",
      "Date                     \n",
      "2004-08-19   44659000.0  \n",
      "2004-08-20   22834300.0  \n",
      "2004-08-23   18256100.0  \n",
      "2004-08-24   15247300.0  \n",
      "2004-08-25    9188600.0  \n",
      "2004-08-26    7094800.0  \n",
      "2004-08-27    6211700.0  \n",
      "2004-08-30    5196700.0  \n",
      "2004-08-31    4917800.0  \n",
      "2004-09-01    9138200.0  \n",
      "2004-09-02   15118600.0  \n",
      "2004-09-03    5152400.0  \n",
      "2004-09-07    5847500.0  \n",
      "2004-09-08    4985600.0  \n",
      "2004-09-09    4061700.0  \n",
      "2004-09-10    8698800.0  \n",
      "2004-09-13    7844100.0  \n",
      "2004-09-14   10828900.0  \n",
      "2004-09-15   10713000.0  \n",
      "2004-09-16    9266300.0  \n",
      "2004-09-17    9472500.0  \n",
      "2004-09-20   10628700.0  \n",
      "2004-09-21    7228700.0  \n",
      "2004-09-22    7581200.0  \n",
      "2004-09-23    8535600.0  \n",
      "2004-09-24    9123400.0  \n",
      "2004-09-27    7066100.0  \n",
      "2004-09-28   16929000.0  \n",
      "2004-09-29   30516400.0  \n",
      "2004-09-30   13758000.0  \n",
      "...                 ...  \n",
      "2018-02-13    1574121.0  \n",
      "2018-02-14    2029979.0  \n",
      "2018-02-15    1806206.0  \n",
      "2018-02-16    1971928.0  \n",
      "2018-02-20    1646405.0  \n",
      "2018-02-21    2024534.0  \n",
      "2018-02-22    1386115.0  \n",
      "2018-02-23    1234539.0  \n",
      "2018-02-26    1489118.0  \n",
      "2018-02-27    2094863.0  \n",
      "2018-02-28    2431023.0  \n",
      "2018-03-01    2766856.0  \n",
      "2018-03-02    2508145.0  \n",
      "2018-03-05    1432369.0  \n",
      "2018-03-06    1169068.0  \n",
      "2018-03-07    1537429.0  \n",
      "2018-03-08    1510478.0  \n",
      "2018-03-09    2070174.0  \n",
      "2018-03-12    2129297.0  \n",
      "2018-03-13    2129435.0  \n",
      "2018-03-14    2033697.0  \n",
      "2018-03-15    1623868.0  \n",
      "2018-03-16    2654602.0  \n",
      "2018-03-19    3076349.0  \n",
      "2018-03-20    2709310.0  \n",
      "2018-03-21    1990515.0  \n",
      "2018-03-22    3418154.0  \n",
      "2018-03-23    2413517.0  \n",
      "2018-03-26    3272409.0  \n",
      "2018-03-27    2940957.0  \n",
      "\n",
      "[3424 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "#%%capture #suppress output\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Difference between \"Adj. High\" and \"Adj. Low\" (margin) for instance tells us something about the volatility**\n",
    "\n",
    "**Difference between \"Adj. Open\" and \"Adj. Close\" tells us by how much the price changed during the day**\n",
    "\n",
    "--> Valuable relationship - however **linear regression** is not going to seek out this.\n",
    "\n",
    "-----\n",
    "\n",
    "What we need to do is to define special relationships and then define those as our features rather than using redundant prices that do not give us anything useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['Adj. Open','Adj. High','Adj. Low','Adj. Close','Adj. Volume',]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define new column for the high minus low - something like the percent volatility.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['HL_PCT'] = (df['Adj. High'] - df['Adj. Low']) / df['Adj. Low'] * 100.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Does not matter if times 100 or not - classifier does not really care (as long as it is normalized)!**\n",
    "\n",
    "------\n",
    "\n",
    "**Now let's define the daily change.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['PCT_change'] = (df['Adj. Close'] - df['Adj. Open']) / df['Adj. Open'] * 100.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now we define columns we really care about**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['Adj. Close','HL_PCT','PCT_change','Adj. Volume']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Adj. Close    HL_PCT  PCT_change  Adj. Volume\n",
      "Date                                                     \n",
      "2004-08-19   50.322842  8.441017    0.324968   44659000.0\n",
      "2004-08-20   54.322689  8.537313    7.227007   22834300.0\n",
      "2004-08-23   54.869377  4.062357   -1.227880   18256100.0\n",
      "2004-08-24   52.597363  7.753210   -5.726357   15247300.0\n",
      "2004-08-25   53.164113  3.966115    1.183658    9188600.0\n"
     ]
    }
   ],
   "source": [
    "print(df.head()) # Dataframe.head(n=5) - this method is used to return top n (5 by default) rows of df or series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**\"Adj. Close\" highly dependent on \"HL_PCT\" and \"PCT_change\" which would be a highly biased classifier - so predicting with \"Adj. Close\" would not really make sense. But what we will do here is to take the last let's say 10 values of \"Adj. Close\" and define that as a feature trying to predict the future value.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "#2 - Regression Features and Labels\n",
    "======"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_col = 'Adj. Close'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In order to replace NaN data you have to put something back in. Usually deleting columns is not a good idea!**\n",
    "**We usually miss data in the real world and want to avoid deleting a whole column!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(-99999, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In pandas, when the data in a column/row is not available, it's labelled NaN(not a number). dropna() removes these \"cells\" and \"inplace=True\" means change it in the dataset and not just the copy of the dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_out = int(math.ceil(0.01*len(df))) # round everything up to the nearest whole / do not forget to import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We are taking 0.01 or 1% of the length of all the rows within the dataframe. Each row in the dataFrame is representative of a day in the life of the stock. So if the stock has been trading for 365 days, there will be 365 rows in the dataFrame. 1% of 365 is 3.65 days which is then rounded up by the math.ceil function to 4 days. The 4 days will be the \"forecast_out\" variable which is the variable that used to shift the \"Adj.Close\" price column in the dataFame up by 4. In other words, if you were standing at day 1 of the stock when it was first traded, the prediction or the \"label\" from his algorithm would tell you that at day 4, your stock will be valued at the amount of the close as taken on day 4 from actual data. This isn't totally useful information since you can look at the Adj.Close column on day 4 to get back to the label info on day 1. This is really all done to build a training set so that the machine can learn from the trend.**\n",
    "\n",
    "\n",
    "----\n",
    "\n",
    "**We can also test later what the difference between \"Adj. Close\" and \"label\" is and when using 10% the difference is much bigger between those two than using 1%. I assume that this is just related to the fact that in a short amount of time the prices changed so drastically before they \"stabilized\" a bit later.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(forecast_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now we are renaming the column so that if you would use this sheet as a template you do not have to adapt every single line seperately. We also shift the forecast column up. This way the label column for each row will be the \"Adj. Close\" price 10 days into the future**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'] = df[forecast_col].shift(-forecast_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Adj. Close    HL_PCT  PCT_change  Adj. Volume      label\n",
      "Date                                                                \n",
      "2004-08-19   50.322842  8.441017    0.324968   44659000.0  69.078238\n",
      "2004-08-20   54.322689  8.537313    7.227007   22834300.0  67.839414\n",
      "2004-08-23   54.869377  4.062357   -1.227880   18256100.0  68.912727\n",
      "2004-08-24   52.597363  7.753210   -5.726357   15247300.0  70.668146\n",
      "2004-08-25   53.164113  3.966115    1.183658    9188600.0  71.219849\n"
     ]
    }
   ],
   "source": [
    "# print(df.head())\n",
    "#print(df.tail())\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First we fill the NaN values with -99999, then after we shift the value, the tail end will have NaN values.\n",
    "Instead of Replacing them with -99999 we drop them. You might think why we did not drop them in the first place. Because In the dataset one or the other row had NaN so if it would be deleted, the dataset would have reduced drastically!!! But in the next we can drop them as only 33 rows will be droped. Why 33? Because it is 1% (0.01) of the hole data set. So the loss in Data was minimal.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "#3 - Training and Testing\n",
    "======"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Features are everything except label column. df.drop returns a new dataframe which is being converted to a numpy array and saved to X (independent variables). Y is our dependent variable!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(df.drop(['label'],1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Before shooting through the classifier we have to scale X!**\n",
    "\n",
    "- **Scale if you want to optimize the accuracy of your model predictions.**\n",
    "- **Do not scale if you want to keep the most interpretation as possible in your model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = preprocessing.scale(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_lately = X[-forecast_out:] # that's what we predict against! Extracting last entry of X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[:-forecast_out:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(df['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is the difference between the training set and the test set?**\n",
    "-----\n",
    "**The training set is a subset of your data on which your model will learn how to predict the dependent variable with the independent variables. The test set is the complimentary subset from the training set, on which you will evaluate your model to see if it manages to predict correctly the dependent variable with the independent variables.**\n",
    "\n",
    "\n",
    "-----\n",
    "\n",
    "- **Training Set:** *The sample of data used to fit the model*. The actual dataset that we use to train the model (weights and biases in the case of Neural Network). The model sees and learns from this data.\n",
    "----\n",
    "- **Validation Set:** *The sample of data used to provide an unbiased evaluation of a model fit on the training dataset while tuning model hyperparameters. The evaluation becomes more biased as skill on the validation dataset is incorporated into the model configuration*. The validation set is used to evaluate a given model, but this is for frequent evaluation.\n",
    "-----\n",
    "- **Test Set:** *The sample of data used to provide an unbiased evaluation of a final model fit on the training dataset.* The Test dataset provides the gold standard used to evaluate the model. It is only used once a model is completely trained (using the train and validation sets). The test set is generally what is used to evaluate competing models (For example on many Kaggle competitions, the validation set is released initially along with the training set and the actual test set is only released when the competition is about to close, and it is the result of the model on the Test set that decides the winner). Many a times the validation set is used as the test set, but it is not good practice. The test set is generally well curated. It contains carefully sampled data that spans the various classes that the model would face, when used in the real world."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2) # deleted \"random_state = 0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define the regressor now!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clf = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = svm.SVR() # performs a lot worse than Linear Regression!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clf.fit(X_train, y_train) # training step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**By creating a so called pickle we do not have to always train our classifier which is kind of useless in this case but sometimes the training can take up Gigabytes which would be an inefficient task to do thus the pickle trick.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('linearregression.pickle', 'wb') as f: # with \"with open...\" the close() function will be autom. called\n",
    " #   pickle.dump(clf, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_in = open('linearregression.pickle', 'rb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Once run you can comment out:**\n",
    "\n",
    "- clf = LinearRegression()\n",
    "\n",
    "- clf.fit(X_train, y_train)\n",
    "\n",
    "- with open('linearregression.pickle', 'wb') as f:\n",
    "    pickle.dump(clf, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = pickle.load(pickle_in) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.97703396571\n"
     ]
    }
   ],
   "source": [
    "print(accuracy) # this is the squared error!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "#4 - Forecasting and Predicting\n",
    "======"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_set = clf.predict(X_lately)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1097.3585024   1070.74036716  1016.2129631   1055.45226624  1071.68416056\n",
      "  1072.47341686  1089.78880161  1108.40689682  1113.76956196  1120.96217446\n",
      "  1130.75596015  1127.89455085  1146.50393287  1162.91346796  1134.28465635\n",
      "  1120.89387017  1085.2099126   1099.85734743  1111.83153911  1120.10875412\n",
      "  1132.69304114  1148.53948905  1178.73386253  1184.53807383  1154.93392242\n",
      "  1167.57687008  1168.27656636  1151.50875155  1116.06467225  1112.96968501\n",
      "  1111.64807517  1067.90882966  1040.32166404  1067.24158995  1016.41100922] 0.97703396571 35\n"
     ]
    }
   ],
   "source": [
    "print(forecast_set, accuracy, forecast_out) # forecast_out shows us the next 35 days of prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Date is not a feature so we have to do some work to get something we can actually plot.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Forecast'] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**\".iloc\" works based on integer positioning. So no matter what our row labels are, we can always, e.g., get the first row by doing df.iloc[0]**\n",
    "\n",
    "-----\n",
    "\n",
    "**.loc selects data only by labels!** --> Good explanation here **https://stackoverflow.com/questions/31593201/how-are-pandas-iloc-ix-and-loc-different-and-related**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_date = df.iloc[-1].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-05 00:00:00\n"
     ]
    }
   ],
   "source": [
    "print(df.iloc[-1].name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_unix = last_date.timestamp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_day = 86400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_unix = last_unix + one_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in forecast_set:\n",
    "    next_date = datetime.datetime.fromtimestamp(next_unix)\n",
    "    next_unix += one_day\n",
    "    df.loc[next_date] = [np.nan for _ in range (len(df.columns)-1)] + [i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To see what this for loop does simply print the tail to get a rough idea**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Adj. Close  HL_PCT  PCT_change  Adj. Volume  label     Forecast\n",
      "Date                                                                       \n",
      "2018-03-08         NaN     NaN         NaN          NaN    NaN  1111.648075\n",
      "2018-03-09         NaN     NaN         NaN          NaN    NaN  1067.908830\n",
      "2018-03-10         NaN     NaN         NaN          NaN    NaN  1040.321664\n",
      "2018-03-11         NaN     NaN         NaN          NaN    NaN  1067.241590\n",
      "2018-03-12         NaN     NaN         NaN          NaN    NaN  1016.411009\n"
     ]
    }
   ],
   "source": [
    "print(df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11a31d358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['Adj. Close'].plot()\n",
    "df['Forecast'].plot()\n",
    "plt.legend(loc=0)\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price')\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "#5 - Pickling and Scaling\n",
    "======\n",
    "\n",
    "- for that we imported **pickle** at the top"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "#6 - How Regression works\n",
    "======\n",
    "\n",
    "Please visit the following page for a rough explanation of regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "#7 - How to program Best Fit Slope\n",
    "======"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
